# Import the needed libraries
import keras
import numpy as np
import pandas as pd
import pycountry
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go
from keras import Sequential
from keras.src.layers import Dense, Dropout, BatchNormalization, LeakyReLU
from sklearn.model_selection import GridSearchCV
from sklearn.tree import _tree
from keras.src.optimizers import Adam
# from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import IsolationForest, GradientBoostingClassifier
# from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
# from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from scikeras.wrappers import KerasClassifier
# from sklearn.ensemble import ExtraTreesClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score

# 1. Loading the Data

# Load the data from a CSV file
paid_invoices = pd.read_csv(r'C:\Users\ibrossi\Desktop\Project\Data\Paid invoices.csv')
payment_terms = pd.read_csv(r'C:\Users\ibrossi\Desktop\Project\Data\Payment terms.csv')

# Display the first few rows of the dataframes
paid_invoices.head(10)
payment_terms.head(10)

# Merge the two tables and take only the days_from_baseline_date_max column
payment_terms.rename(columns={'payment_terms_ZTERM': 'PayT_Code'}, inplace=True)
invoices = paid_invoices.merge(payment_terms[['PayT_Code', 'days_from_baseline_date_max']], on='PayT_Code', how='left')
invoices.rename(columns={'days_from_baseline_date_max': 'Payment_Terms_Days'}, inplace=True)

# 2. Initial Data Analysis
# 2.1. Dataset Overview

# Print the dimensions of the dataframes, showing the number of rows and columns.
print(invoices.shape)  # 836864 rows, 35 columns

# Check data types and names for each column
invoices.info()
print(invoices.columns)

# 2.2. Summary Statistics

# Summary statistics for numerical variables
check = invoices.describe().T

# Summary statistics for categorical variables
check2 = invoices.describe(include='object').T

# 3. Data Cleaning & Transformation
# 3.1. Handling Missing Values
missing_values = invoices.isnull().sum()

# 1. PayT_Code - 20863 missing values
# 5,283 documents issued to payers with code starting with '3' which are cash customers so for all of these documents the payment term will be replaced with code CADS, which represents cash payments
invoices.loc[invoices['PayT_Code'].isna() & invoices['Payer_Code'].astype(str).str.startswith('3'), 'Payment_Terms_Days'] = 0
invoices.loc[invoices['PayT_Code'].isna() & invoices['Payer_Code'].astype(str).str.startswith('3'), 'PayT_Code'] = 'CADS'

# the rest 15580 documents will be removed for the dataframe as this variable is important
invoices = invoices[invoices['PayT_Code'].notna()]

# 2. DunningBlock_MANSP - 803891 missing values - these entries will be replaced with "No Dunning Block", which later will be used to encode the column accordingly.
invoices['DunningBlock_MANSP'] = invoices['DunningBlock_MANSP'].fillna('No Dunning Block')

# 3. LastDunning_Level - 703758 missing values - these entries will be replaced with "No Dunning", which later will be used to encode the column accordingly.
invoices['LastDunning_Level'] = invoices['LastDunning_Level'].fillna(0)

# 4. LastDunning_RunDate - 703758 missing values - we will drop this column from the dataframe??
invoices = invoices.drop(columns=['LastDunning_RunDate'])

# 5. Comment_Created_On - 361407 missing values - we will drop this column from the dataframe??
invoices = invoices.drop(columns=['Comment_Created_On'])

# 5. Customer_Contact_Type_description - 361407 missing values - these entries will be replaced with "Not Contacted", which later will be used to encode the column accordingly.
invoices['Customer_Contact_Type_description'] = invoices['Customer_Contact_Type_description'].fillna('Not Contacted')

# 6. Customer_Contact_Result_description - 361407 missing values - these entries will be replaced with "Not Contacted", which later will be used to encode the column accordingly.
invoices['Customer_Contact_Result_description'] = invoices['Customer_Contact_Result_description'].fillna('Not Contacted')

# 7. Comment - 361407 missing values - these entries will be replaced with "Not Contacted", which later will be used to encode the column accordingly.
invoices['Comment'] = invoices['Comment'].fillna('Not Contacted')

# 8. AccountChannel_KATR2 - we will drop this column from the dataframe
invoices = invoices.drop(columns=['AccountChannel_KATR2'])

# 9. AccountChannel_Description - 11723 missing values - these entries will be replaced with "No Channel", which later will be used to encode the column accordingly.
invoices['AccountChannel_Description'] = invoices['AccountChannel_Description'].fillna('No Channel')

# 10. Country_LAND1 - 6 missing values - these documents will be removed for the dataframe
invoices = invoices[invoices['Country_LAND1'].notna()]

# 11. Dispute_Category_Description - 811951 missing values - these entries will be replaced with "No Dispute", which later will be used to encode the column accordingly.
invoices['Dispute_Category_Description'] = invoices['Dispute_Category_Description'].fillna('No Dispute')

# 12. Dispute_Reason_Description - 811951 missing values - these entries will be replaced with "No Dispute", which later will be used to encode the column accordingly.
invoices['Dispute_Reason_Description'] = invoices['Dispute_Reason_Description'].fillna('No Dispute')

# 13. DisputeStatus - we will drop this column from the dataframe
invoices = invoices.drop(columns=['DisputeStatus'])

# 14. State_Description - 750506 missing values - these entries will be replaced with "Not Contacted", which later will be used to encode the column accordingly.
invoices['State_Description'] = invoices['State_Description'].fillna('Not Contacted')

# 15. Credit_Limit - 65 missing values - these documents will be removed for the dataframe
invoices = invoices[invoices['Credit_Limit'].notna()]

# 17. Interest_Flag - 784147 missing values - these entries will be replaced with "No Interest Charged", which later will be used to encode the column accordingly.
invoices.loc[:, 'Interest_Flag'] = invoices['Interest_Flag'].fillna('No Interest Charged')

# 18. Interest_Rate - 784147 missing values - these entries will be replaced with 0, which later will be used to encode the column accordingly.
invoices.loc[:, 'Interest_Rate'] = invoices['Interest_Rate'].fillna(0)

# Reset the index
invoices = invoices.reset_index(drop=True)

# 3.2. Correcting the columns types
invoices.info()

# Convert the columns to appropriate data format
# Object type to string type
invoices['Company_Code'] = invoices['Company_Code'].astype('string')
invoices['Group_Currency'] = invoices['Group_Currency'].astype('string')
invoices['PayT_Code'] = invoices['PayT_Code'].astype('string')
invoices['Timeliness_Description'] = invoices['Timeliness_Description'].astype('string')
invoices['Unique_Key'] = invoices['Unique_Key'].astype('string')
invoices['DunningBlock_MANSP'] = invoices['DunningBlock_MANSP'].astype('string')
invoices['Overdue_Flag'] = invoices['Overdue_Flag'].astype('string')
invoices['Dunning_Flag'] = invoices['Dunning_Flag'].astype('string')
invoices['Customer_Contact_Type_description'] = invoices['Customer_Contact_Type_description'].astype('string')
invoices['Customer_Contact_Result_description'] = invoices['Customer_Contact_Result_description'].astype('string')
invoices['Comment'] = invoices['Comment'].astype('string')
invoices['Comment_Flag'] = invoices['Comment_Flag'].astype('string')
invoices['Effect_Flag'] = invoices['Effect_Flag'].astype('string')
invoices['AccountChannel_Description'] = invoices['AccountChannel_Description'].astype('string')
invoices['Country_LAND1'] = invoices['Country_LAND1'].astype('string')
invoices['Dispute_Category_Description'] = invoices['Dispute_Category_Description'].astype('string')
invoices['Dispute_Reason_Description'] = invoices['Dispute_Reason_Description'].astype('string')
invoices['State_Description'] = invoices['State_Description'].astype('string')
invoices['Interest_Flag'] = invoices['Interest_Flag'].astype('string')

# Object type to int type
invoices['LastDunning_Level'] = invoices['LastDunning_Level'].astype('int64')

# Object type to date type
invoices['Document_Date'] = pd.to_datetime(invoices['Document_Date'])
invoices['Posting_Date'] = pd.to_datetime(invoices['Posting_Date'])
invoices['Clearing_Date'] = pd.to_datetime(invoices['Clearing_Date'])
invoices['DueDate'] = pd.to_datetime(invoices['DueDate'])

# Check the data types to confirm changes
invoices.info()

# 3.3. Handling Duplicates
# Check for duplicates based on the Unique document key as this is unique identifier for each invoice
duplicate_count = invoices['Unique_Key'].duplicated().sum()  # 8168

# Remove the duplicates
# Sort by Clearing_Date (latest dates first)
invoices = invoices.sort_values(by='Clearing_Date', ascending=False)

# Remove duplicates based on Unique_Key, keeping the latest Clearing_Date
invoices_cleaned = invoices.drop_duplicates(subset=['Unique_Key'], keep='first')

# 3.4. Treating credit note documents or documents with 0 total amount

# Filter the dataframe to include only rows where the amount column is less than or equal to 0
credit_notes = invoices_cleaned[invoices_cleaned['Amount_GC'] <= 0]  # 41 808 credit notes

# Finding the percentage of credit notes - 5.05%
print(f"The percentage of credit notes in the dataset is:"
      f" {(credit_notes.shape[0] / invoices_cleaned.shape[0]) * 100:.2f}%")

# These documents will be removed from the dataframe as they were not paid by the customers - they were refunded back to the clients
invoices_cleaned = invoices_cleaned[~(invoices_cleaned['Amount_GC'] <= 0)]

# Reset the index
invoices_cleaned = invoices_cleaned.reset_index(drop=True)

# 3.5. Treating documents with Posting date after Due Date

# Filter the dataframe to include only rows the Posting Date is bigger that the Due Date
problematic_invoices = invoices_cleaned[(invoices_cleaned['Posting_Date'] > invoices_cleaned['DueDate'])]

# Finding the percentage of these invoices -
print(f"The percentage of invoices with Due Date before Posting Date in the dataset is:"
      f" {(problematic_invoices.shape[0] / invoices_cleaned.shape[0]) * 100:.2f}%")

# These documents will be removed from the dataframe as never can be paid on time and most probably they have been transferd from another system/company code
invoices_cleaned = invoices_cleaned[~(invoices_cleaned['Posting_Date'] > invoices_cleaned['DueDate'])]

# Reset the index
invoices_cleaned = invoices_cleaned.reset_index(drop=True)

# 3.6. Treating documents with payment in advance (PADV payment terms code)

# Filter the dataframe to include only rows where payment term is PADV
padv_invoices = invoices_cleaned[(invoices_cleaned['PayT_Code'] == 'PADV')]

# Finding the percentage of paid in advance invoices
print(f"The percentage of invoices with PADV payment terms in the dataset is:"
      f" {(padv_invoices.shape[0] / invoices_cleaned.shape[0]) * 100:.2f}%")

# These documents will be removed from the dataframe as they should be paid in advance
invoices_cleaned = invoices_cleaned[~(invoices_cleaned['PayT_Code'] == 'PADV')]

# Reset the index
invoices_cleaned = invoices_cleaned.reset_index(drop=True)

# 4. Visual exploration of the data
# 4.1. Investigating Timeliness_Description Column

# Categories are ordered from earliest to latest payment
category_order = {'Before DueDate':0, 'On DueDate':0, 'Due 1-7 days':0, 'Due 8-30 days':0, 'Due >30 days':0}

# Calculate the value counts and reindex to ensure the correct order
timeliness_counts = invoices_cleaned['Timeliness_Description'].value_counts()

# Update the counts in category_order
for category, count in timeliness_counts.items():
    if category in category_order:  # Check if the category exists in category_order
        category_order[category] = count

# Define a color scale: Green for early, Yellow for on time, Red for late
colors = ['#2ecc71', '#27ae60', '#f1c40f', '#e67e22', '#e74c3c']

# Plot the bar chart
fig = px.bar(
    x=category_order.keys(),
    y=category_order.values(),
    labels={'x': 'Timeliness Category', 'y': 'Number of Transactions'},
    title='Payment Timeliness Categories',
    color=timeliness_counts.index,
    color_discrete_sequence=colors,
    text_auto=True
)
fig.show()

# Timeliness Category by Invoice Amount
# Sum Amount_GC for each Timeliness_Description category
timeliness_amounts = invoices_cleaned.groupby('Timeliness_Description')['Amount_GC'].sum()

# Convert amounts to millions
timeliness_amounts = (timeliness_amounts / 1000000).reindex(category_order, fill_value=0)  # Ensure correct order

# Define color scale: Green for early, Yellow for on time, Red for late
colors = ['#2ecc71', '#27ae60', '#f1c40f', '#e67e22', '#e74c3c']

# Plot the bar chart (showing Amount_GC in Millions)
fig = px.bar(
    x=timeliness_amounts.index,
    y=timeliness_amounts.values,
    labels={'x': 'Timeliness Category', 'y': 'Total Invoice Amount (Millions)'},
    title='Payment Timeliness Categories (By Amount in Millions)',
    color=timeliness_amounts.index,
    color_discrete_sequence=colors,
    text=timeliness_amounts.values  # Ensure correct text is assigned
)

# Fix text display issue and format numbers in millions
fig.update_traces(texttemplate='%{text:.2f}M', textposition='outside')

fig.update_layout(
    xaxis_title='Timeliness Category',
    yaxis_title='Total Invoice Amount (Millions)'
)

fig.show()

# 4.2. Investigating Overdue_Flag Column

# Calculate the value counts and reindex to ensure the correct order
overdue_counts = invoices_cleaned['Overdue_Flag'].value_counts()

# Compute percentages
total_count = overdue_counts.sum()
percentages = (overdue_counts / total_count) * 100

# Define colors
colors = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create the bar chart
fig = px.bar(
    x=overdue_counts.index,
    y=overdue_counts.values,
    labels={'x': 'Category', 'y': 'Number of Transactions'},
    title='Payment on time',
    color=overdue_counts.index,
    color_discrete_map=colors,
    text=[f"{v:.1f}%" for v in percentages]  # Display percentage on bars
)
fig.show()

# Overdue flag  by month

# Group by month and overdue flag
monthly_counts = invoices_cleaned.groupby([
    invoices_cleaned['Clearing_Date'].dt.to_period('M'), 'Overdue_Flag'
]).size().unstack().fillna(0)

# Convert period to string for plotting
monthly_counts.index = monthly_counts.index.astype(str)

# Define colors
colors = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create the line chart
fig = px.line(
    monthly_counts,
    x=monthly_counts.index,
    y=monthly_counts.columns,
    labels={'x': 'Month', 'y': 'Number of Transactions'},
    title='Overdue vs. Not Overdue Documents Over Time',
    color_discrete_map=colors
)
fig.update_layout(yaxis=dict(range=[0, monthly_counts.values.max() * 1.1]))
fig.show()

# 4.3. Investigating Amount_GC Column
# Plot histogram of Amount_GC
fig = px.histogram(
    invoices_cleaned,
    x='Amount_GC',
    nbins=20,
    title='Distribution of Transaction Amounts',
    labels={'Amount_GC': 'Amount (Group Currency)'},
    template='plotly',
)
fig.update_traces(marker_color='#27ae60')
fig.update_layout(xaxis_title='Amount (Group Currency)', yaxis_title='Frequency')
fig.show()
# The amount GC column has a highly right-skewed distribution (a long tail)

# Plot histogram of log-transformed values
invoices_cleaned['Log_Amount_GC'] = np.log1p(invoices_cleaned['Amount_GC'])

fig = px.histogram(
    invoices_cleaned,
    x='Log_Amount_GC',
    nbins=50,
    title='Distribution of Log-Transformed Transaction Amounts',
    labels={'Log_Amount_GC': 'Log(Amount (Group Currency))'},
    template='plotly',
)
fig.update_traces(marker_color='#27ae60')
fig.update_layout(xaxis_title='Log(Amount (Group Currency))', yaxis_title='Frequency')
fig.show()

# Plot box plot for Amount_GC
# Compute box plot statistics
q1 = invoices_cleaned['Amount_GC'].quantile(0.25)
q3 = invoices_cleaned['Amount_GC'].quantile(0.75)
median = invoices_cleaned['Amount_GC'].median()
mean = invoices_cleaned['Amount_GC'].mean()
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Create the box plot
fig = go.Figure()

# Add the main box plot (green)
fig.add_trace(go.Box(
    y=invoices_cleaned['Amount_GC'],
    name="Transactions",
    marker_color='#27ae60',  # Green box plot
    boxpoints='outliers',  # Show outliers
))

# Add statistics on the right side
stats_text = f"""
<b>Statistics:</b><br>
Median: {median:,.2f}<br>
Mean: {mean:,.2f}<br>
Q1: {q1:,.2f}<br>
Q3: {q3:,.2f}<br>
IQR: {iqr:,.2f}<br>
Lower Bound: {lower_bound:,.2f}<br>
Upper Bound: {upper_bound:,.2f}
"""

fig.add_annotation(
    text=stats_text,
    x=1.05, y=0.5,  # Position on the right side
    xref="paper", yref="paper",
    showarrow=False,
    font=dict(size=14),
    align="left",
    bordercolor="#2c3e50",
    borderwidth=2,
    bgcolor="white",
    opacity=0.8
)

# Update layout
fig.update_layout(
    title='Box Plot of Transaction Amounts',
    yaxis_title='Amount (Group Currency)',
    xaxis=dict(visible=False),
    template='plotly'
)
fig.show()

# A few outliers are visible with extremely high invoice amounts

# Does the amount matter if the invoice is paid on time
# Filter invoices with Amount_GC > Q3
high_value_invoices = invoices_cleaned[invoices_cleaned['Amount_GC'] > 500000]

# Define colors based on Overdue_Flag
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create scatter plot
fig = px.scatter(
    high_value_invoices,
    x='Timeliness',
    y='Amount_GC',
    color='Overdue_Flag',
    title=f'Scatter Plot of High-Value Invoices (Above 500,000 EUR)',
    labels={'Amount_GC': 'Amount (Group Currency)', 'Timeliness': 'Timeliness'},
    color_discrete_map=color_map
)
fig.show()

# 4.4. Investigating Overdue payments by Company Code

# Top 15 Companies with Overdue Payments
# Filter for overdue payments
overdue_df = invoices_cleaned[invoices_cleaned['Overdue_Flag'] == 'Overdue']

# Group by Company_Code and count overdue payments
overdue_counts = overdue_df['Company_Code'].value_counts().head(15)  # Top 10 companies

# Plot bar chart
fig = px.bar(
    x=overdue_counts.index,
    y=overdue_counts.values,
    title='Top 15 Companies with Overdue Payments',
    labels={'x': 'Company Code', 'y': 'Number of Overdue Payments'},
    text=overdue_counts.values
)
fig.update_traces(textposition='outside')
fig.update_layout(xaxis_title='Company Code', yaxis_title='Number of Overdue Payments')
fig.show()

# Top 15 Companies with Overdue Invoices (Percentage of Total Invoices)

# Total invoices and total amounts by company
total_invoices = invoices_cleaned.groupby('Company_Code').size()
total_amounts= invoices_cleaned.groupby('Company_Code')['Amount_GC'].sum()

# Overdue invoices and overdue amounts by company
overdue_invoices = overdue_df.groupby('Company_Code').size()
overdue_amounts = overdue_df.groupby('Company_Code')['Amount_GC'].sum()

# Calculate percentages
invoice_percentage = (overdue_invoices / total_invoices * 100).fillna(0)
amount_percentage = (overdue_amounts / total_amounts * 100).fillna(0)

# Combine into a DataFrame and get the top 10 companies by invoice percentage
percentage_df = (
    pd.DataFrame({'Invoice_Percentage': invoice_percentage, 'Amount_Percentage': amount_percentage})
    .sort_values(by='Invoice_Percentage', ascending=False)
    .head(15)
)

# Plot bar chart for overdue amount percentage
fig = px.bar(
    percentage_df,
    x=percentage_df.index,
    y='Amount_Percentage',
    title='Top 15 Companies with Overdue Invoices (Percentage of Total Amount)',
    labels={'x': 'Company Code', 'Amount_Percentage': 'Overdue Amount (%)'},
    text='Amount_Percentage',
    template='plotly'
)
fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')
fig.update_layout(xaxis_title='Company Code', yaxis_title='Overdue Amount (%)')
fig.show()

# Top 15 Company Codes by Overdue amount as percentage from the total
# Total amount for the entire dataset
total_amount = invoices_cleaned['Amount_GC'].sum()

# Total overdue amount by company
overdue_amounts = overdue_df.groupby('Company_Code')['Amount_GC'].sum()

# Calculate the percentage of overdue amounts based on the total dataset amount
amount_percentage = (overdue_amounts / total_amount) * 100

# Create a DataFrame and get the top 15 companies by overdue amount percentage
percentage_df = (
    pd.DataFrame({'Amount_Percentage': amount_percentage})
    .sort_values(by='Amount_Percentage', ascending=False)
    .head(15)
)

# Define color scale (red → green gradient)
colorscale = ['#27ae60', '#f39c12', '#e74c3c']  # Red → Orange → Green

# Plot bar chart with color based on Amount_Percentage
fig = px.bar(
    percentage_df,
    x=percentage_df.index,
    y='Amount_Percentage',
    title='Top 15 Companies by Overdue Amounts (Percentage of Total Amount)',
    labels={'x': 'Company Code', 'Amount_Percentage': 'Overdue Amount (%)'},
    text='Amount_Percentage',
    template='plotly',
    color=percentage_df['Amount_Percentage'],  # Use the percentage value for color mapping
    color_continuous_scale=colorscale  # Apply the color gradient
)
fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')

# Update layout
fig.update_layout(
    xaxis_title='Company Code',
    yaxis_title='Overdue Amount (%)',
    coloraxis_colorbar=dict(title="Overdue %")  # Color bar label
)
fig.show()


# 4.5. Investigating the distribution of column Timeliness

# Plot histogram of Timeliness
fig = px.histogram(
    invoices_cleaned,
    x='Timeliness',
    title='Distribution of Timeliness',
    labels={'Timeliness': 'Timeliness (Days)'},
    template='plotly'
)
fig.update_traces(marker_color='#27ae60')
fig.update_layout(
    xaxis_title='Timeliness (Days)',
    yaxis_title='Frequency'
)
fig.show()

# Plot box plot for Timeliness
# Compute statistics
q1 = invoices_cleaned['Timeliness'].quantile(0.25)
q3 = invoices_cleaned['Timeliness'].quantile(0.75)
median = invoices_cleaned['Timeliness'].median()
mean = invoices_cleaned['Timeliness'].mean()
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
min_value = invoices_cleaned['Timeliness'].min()
max_value = invoices_cleaned['Timeliness'].max()

# Create the box plot
fig = go.Figure()

# Add main box plot (green)
fig.add_trace(go.Box(
    y=invoices_cleaned['Timeliness'],
    name="Timeliness",
    marker_color='#27ae60',  # Green box plot
    boxpoints='outliers',  # Show outliers
    marker=dict(outliercolor='#e74c3c')  # Red outliers
))

# Add statistics as a text box on the right
stats_text = f"""
<b>Statistics:</b><br>
Median: {median:,.2f}<br>
Mean: {mean:,.2f}<br>
Q1: {q1:,.2f}<br>
Q3: {q3:,.2f}<br>
IQR: {iqr:,.2f}<br>
Min: {min_value:,.2f}<br>
Max: {max_value:,.2f}<br>
Lower Bound: {lower_bound:,.2f}<br>
Upper Bound: {upper_bound:,.2f}
"""

fig.add_annotation(
    text=stats_text,
    x=1.05, y=0.5,  # Positioning
    xref="paper", yref="paper",
    showarrow=False,
    font=dict(size=14),
    align="left",
    bordercolor="#2c3e50",
    borderwidth=2,
    bgcolor="white",
    opacity=0.8
)

# Update layout
fig.update_layout(
    title='Box Plot of Timeliness Days',
    yaxis_title='Timeliness Days',
    xaxis=dict(visible=False),
    template='plotly'
)

fig.show()

# 4.6. Investigating the seasonal trends in the cash flow

# Group by month and count invoices
monthly_transactions = invoices_cleaned.groupby(invoices_cleaned['Clearing_Date'].dt.to_period('M')).size()

# Create time series plot
fig = px.line(
    x=monthly_transactions.index.astype(str),
    y=monthly_transactions.values,
    title='Trend of Paid Invoices Over Time',
    labels={'x': 'Month', 'y': 'Number of Transactions'}
)

# Set Y-axis to start from 0 and change the line color to green
fig.update_traces(line=dict(color='#27ae60'))
fig.update_layout(
    xaxis_title='Month',
    yaxis_title='Number of Transactions',
    yaxis=dict(range=[0, max(monthly_transactions.values) * 1.1])
)

fig.show()

# Group by month and calculate the sum of Amount_GC
monthly_amounts = invoices_cleaned.groupby(invoices_cleaned['Clearing_Date'].dt.to_period('M'))['Amount_GC'].sum()

# Plot time series
fig = px.line(
    x=monthly_amounts.index.astype(str),
    y=monthly_amounts.values,
    title='Trend of Paid Invoice Amounts Over Time',
    labels={'x': 'Month', 'y': 'Total Invoice Amount (Group Currency)'}
)
fig.update_layout(
    xaxis_title='Month',
    yaxis_title='Total Invoice Amount (Group Currency)',
    xaxis=dict(tickangle=45),
    yaxis=dict(range=[0, max(monthly_amounts.values) * 1.1])
)
fig.update_traces(line=dict(color='#27ae60'))
fig.show()


# 4.7. Payment Timeliness by Amount
# Define custom colors
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create scatter plot
fig = px.scatter(
    invoices_cleaned,
    x='Amount_GC',
    y='Timeliness',
    color='Overdue_Flag',
    title='Payment Timeliness by Invoice Amount',
    labels={'Amount_GC': 'Invoice Amount (Group Currency)', 'Timeliness': 'Days Late/Early'},
    color_discrete_map=color_map  # Assign custom colors
)
fig.update_layout(
    xaxis_title='Invoice Amount (Group Currency)',
    yaxis_title='Timeliness (Days)'
)
fig.show()

# 4.7. Payment Term Days by Amount
# Define custom colors
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create scatter plot
fig = px.scatter(
    invoices_cleaned,
    x='Payment_Terms_Days',
    y='Amount_GC',
    color='Overdue_Flag',
    title='Payment Term Days by Invoice Amount',
    labels={'Amount_GC': 'Invoice Amount (Group Currency)', 'Payment Term Days': 'Days Late/Early'},
    color_discrete_map=color_map  # Assign custom colors
)
fig.update_layout(
    xaxis_title='Invoice Amount (Group Currency)',
    yaxis_title='Payment Terms (Days)'
)
fig.show()


# 4.8. Timeliness Distribution by Payment Term

fig = px.box(
    invoices_cleaned,
    x='PayT_Code',
    y='Timeliness',
    color='Overdue_Flag',
    title='Timeliness Distribution by Payment Term',
    labels={'PayT_Code': 'Payment Term Code', 'Timeliness': 'Days Late/Early'}
)
fig.update_layout(xaxis_title='Payment Term Code', yaxis_title='Timeliness (Days)')
fig.show()

# 4.9. Late Payments by Country

# Convert Country_LAND1 to ISO-3 codes
def convert_to_iso3(alpha2):
    try:
        return pycountry.countries.get(alpha_2=alpha2).alpha_3
    except AttributeError:
        return None

invoices_cleaned['Country_ISO3'] = invoices_cleaned['Country_LAND1'].apply(convert_to_iso3)

# Calculate percentage of late payments by country
late_payments = invoices_cleaned[invoices_cleaned['Overdue_Flag'] == 'Overdue'].groupby('Country_ISO3').size()
total_payments = invoices_cleaned.groupby('Country_ISO3').size()
late_payment_percentage = (late_payments / total_payments * 100).fillna(0)

# Convert the late_payment_percentage Series to a DataFrame for mapping
late_payment_df = late_payment_percentage.reset_index()
late_payment_df.columns = ['Country', 'Late Payment Percentage']
late_payment_df.to_csv(r'C:\Users\ibrossi\Desktop\Project\Data\Late payment percentage by country.csv', index=True)


fig = px.choropleth(
    late_payment_df,
    locations="Country",
    locationmode="ISO-3",  # Use ISO-3 country codes
    color="Late Payment Percentage",
    hover_name="Country",
    color_continuous_scale="Reds",
    title="Percentage of Late Payments by Country"
)

fig.update_layout(
    geo=dict(
        showframe=False,
        showcoastlines=True,
        projection_type='equirectangular'
    )
)
fig.show()

# 4.10. Percentage of total Payments by Country
# Calculate percentage of total payments by country
all_invoices = invoices_cleaned['Unique_Key'].count()
total_payments = invoices_cleaned.groupby('Country_ISO3').size()
payment_percentage_by_country = (total_payments / all_invoices * 100).fillna(0)

# Convert the late_payment_percentage Series to a DataFrame for mapping
payment_percentage_by_country_df = payment_percentage_by_country.reset_index()
payment_percentage_by_country_df.columns = ['Country', 'Invoice Percentage']
payment_percentage_by_country_df.to_csv(r'C:\Users\ibrossi\Desktop\Project\Data\Invoice percentage by country.csv', index=True)


fig = px.choropleth(
    payment_percentage_by_country_df,
    locations="Country",
    locationmode="ISO-3",  # Use ISO-3 country codes
    color="Invoice Percentage",
    hover_name="Country",
    color_continuous_scale="Reds",
    title="Percentage of Invoices by Country",
)

fig.update_layout(
    geo=dict(
        showframe=False,
        showcoastlines=True,
        projection_type='equirectangular'
    )
)
fig.show()

# 4.11. Overdue Invoices by Payment Term

overdue_percentage = invoices_cleaned[invoices_cleaned['Overdue_Flag'] == 'Overdue'].groupby('PayT_Code').size() / invoices_cleaned.groupby('PayT_Code').size() * 100

fig = px.bar(
    x=overdue_percentage.index,
    y=overdue_percentage.values,
    title='Overdue Percentage by Payment Term',
    labels={'x': 'Payment Term Code', 'y': 'Overdue Percentage (%)'},
    text=overdue_percentage.values
)
fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')
fig.update_traces(marker_color='#27ae60')
fig.update_layout(xaxis_title='Payment Term Code', yaxis_title='Overdue Percentage (%)')
fig.show()

# 4.12. Average Payment Timeliness Over Time

# Group by date and calculate average timeliness
avg_timeliness_by_date = invoices_cleaned.groupby(invoices_cleaned['Clearing_Date'].dt.to_period('D'))['Timeliness'].mean()

fig = px.line(
    x=avg_timeliness_by_date.index.astype(str),
    y=avg_timeliness_by_date.values,
    title='Average Payment Timeliness Over Time',
    labels={'x': 'Date', 'y': 'Average Timeliness (Days)'}
)
fig.update_traces(marker_color='#27ae60')
fig.update_layout(xaxis_title='Date', yaxis_title='Average Timeliness (Days)')
fig.show()

# 4.13. Invoices with dunning letters - Overdue / Not Overdue

# Filter only invoices with Dunning_Flag = Yes
dunning_invoices = invoices_cleaned[invoices_cleaned['Dunning_Flag'] == 'Yes']

# Count occurrences of Overdue vs. Not Overdue
dunning_counts = dunning_invoices.groupby('Overdue_Flag').size().reset_index(name='Count')

# Define custom colors
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create stacked bar chart
fig = px.bar(
    dunning_counts,
    x='Overdue_Flag',
    y='Count',
    title='Invoices with dunning letters (Overdue vs. Not Overdue)',
    labels={'Overdue_Flag': 'Status', 'Count': 'Number of Invoices'},
    color='Overdue_Flag',
    color_discrete_map=color_map,
    text='Count'
)

# Adjust layout
fig.update_layout(
    xaxis_title='Invoice Status',
    yaxis_title='Number of Invoices'
)

fig.show()

# 4.14. Overdue vs. Not Overdue invoices by contact type - invoice count
# Count occurrences of Overdue vs. Not Overdue per Customer_Contact_Type_Description
contact_counts = invoices_cleaned.groupby(['Customer_Contact_Type_description', 'Overdue_Flag']).size().reset_index(name='Count')

# Define custom colors
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create grouped bar chart
fig = px.bar(
    contact_counts,
    x='Customer_Contact_Type_description',
    y='Count',
    title='Overdue vs. Not Overdue Invoices by Customer Contact Type',
    labels={'Customer_Contact_Type_Description': 'Customer Contact Type', 'Count': 'Number of Invoices'},
    color='Overdue_Flag',
    barmode='group',  # Group bars side by side
    color_discrete_map=color_map,  # Custom colors
    text='Count'  # Show numbers on bars
)

# Adjust layout for better readability
fig.update_traces(texttemplate='%{text}', textposition='outside')
fig.update_layout(
    xaxis_title='Customer Contact Type',
    yaxis_title='Number of Invoices'
)

fig.show()

# 4.14. Overdue vs. Not Overdue invoices by contact type - sum of invoice amounts
# Sum Amount_GC for Overdue vs. Not Overdue per Customer_Contact_Type_Description
contact_amounts = invoices_cleaned.groupby(['Customer_Contact_Type_description', 'Overdue_Flag'])['Amount_GC'].sum().reset_index()

# Convert values to millions
contact_amounts['Amount_GC'] = contact_amounts['Amount_GC'] / 1000000

# Define custom colors
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create grouped bar chart (summed invoice amounts in millions)
fig = px.bar(
    contact_amounts,
    x='Customer_Contact_Type_description',
    y='Amount_GC',
    title='Overdue vs. Not Overdue Amounts by Customer Contact Type (in Millions)',
    labels={'Customer_Contact_Type_Description': 'Customer Contact Type', 'Amount_GC': 'Total Invoice Amount (Millions)'},
    color='Overdue_Flag',
    barmode='group',  # Group bars side by side
    color_discrete_map=color_map,  # Custom colors
    text='Amount_GC'  # Show values on bars
)

# Adjust layout for better readability
fig.update_traces(texttemplate='%{text:.2f}M', textposition='outside')  # Show values in Millions with 'M' suffix
fig.update_layout(
    xaxis_title='Customer Contact Type',
    yaxis_title='Total Invoice Amount (Millions)'
)

fig.show()

# 4.14. Disputed invoices - Overdue / Not Overdue

# Filter only invoices with Disputes
disputed_invoices = invoices_cleaned[invoices_cleaned['Dispute_Category_Description'] != 'No Dispute']

# Count occurrences of Overdue vs. Not Overdue
disputed_counts = disputed_invoices.groupby('Overdue_Flag').size().reset_index(name='Count')

# Define custom colors
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create stacked bar chart
fig = px.bar(
    disputed_counts,
    x='Overdue_Flag',
    y='Count',
    title='Disputed Invoices (Overdue vs. Not Overdue)',
    labels={'Overdue_Flag': 'Status', 'Count': 'Number of Invoices'},
    color='Overdue_Flag',
    color_discrete_map=color_map,
    text='Count'
)

# Adjust layout
fig.update_layout(
    xaxis_title='Invoice Status',
    yaxis_title='Number of Invoices'
)

fig.show()

# 5. Outlier Detection and Treatment

# Initializing the IsolationForest model with a contamination parameter of 0.01
model = IsolationForest(n_estimators=100, contamination=0.005, random_state=42)

# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)
outliers_testing_invoices = invoices_cleaned[['Amount_GC', 'Timeliness']]
outliers_testing_invoices['Outlier_Scores'] = model.fit_predict(outliers_testing_invoices.iloc[:, 1:].to_numpy())

# Creating a new column to identify outliers (1 for inliers and -1 for outliers)
outliers_testing_invoices['Is_Outlier'] = [1 if x == -1 else 0 for x in outliers_testing_invoices['Outlier_Scores']]

# Create bar chart
# Calculate the percentage of inliers and outliers
outlier_percentage = outliers_testing_invoices['Is_Outlier'].value_counts(normalize=True) * 100
labels = outlier_percentage.index.map({0: 'Not Outlier', 1: 'Outlier'})

# Less than 1% of the customers have been identified as outliers in our dataset.
# This percentage seems to be a reasonable proportion, not too high to lose a significant
# amount of data, and not too low to retain potentially noisy data points.

# Merge the invoices table with the outliers table
invoices_cleaned = invoices_cleaned.merge(outliers_testing_invoices[['Is_Outlier']], left_index=True, right_index=True, how='inner')

# Separate the outliers for analysis
outliers_data = invoices_cleaned[invoices_cleaned['Is_Outlier'] == 1]
outliers_data.to_csv(r'C:\Users\ibrossi\Desktop\Project\Data\Outliers_table.csv', index=True)

# Remove the outliers from the main dataset
invoices_cleaned = invoices_cleaned[invoices_cleaned['Is_Outlier'] == 0]

# Drop the 'Is_Outlier' columns
invoices_cleaned = invoices_cleaned.drop(columns=['Is_Outlier'])

# Reset the index of the cleaned data
invoices_cleaned.reset_index(drop=True, inplace=True)

# Remove all invoices with Amount_GC > 1000000 as they also are outliers but the Isolation Forest model does not recognize them as such
invoices_cleaned = invoices_cleaned[~(invoices_cleaned['Amount_GC'] >= 1000000)]

# Reset the index
invoices_cleaned = invoices_cleaned.reset_index(drop=True)

# 6. Splitting the data into train, validation and test dataframes
# Sort the data by Clearing_Date in ascending order
invoices_cleaned = invoices_cleaned.sort_values(by='Clearing_Date')

# Calculate the number of rows for each split
n = len(invoices_cleaned)
train_size = int(n * 0.8)  # 80% for training
validation_size = int(n * 0.1)  # 10% for validation

# Create train, validation, and test sets
train_set = invoices_cleaned.iloc[:train_size]  # First 80% for training
validation_set = invoices_cleaned.iloc[train_size:train_size + validation_size]  # Next 10% for validation
test_set = invoices_cleaned.iloc[train_size + validation_size:]  # Last 10% for testing

# Print the sizes of the splits
print(f"Train set: {len(train_set)} rows")
print(f"Validation set: {len(validation_set)} rows")
print(f"Test set: {len(test_set)} rows")

# Visualize the distribution of paid on time and not paid on time invoices for the three data sets
# Calculate the distribution of Overdue_Flag for each set
train_distribution = train_set['Overdue_Flag'].value_counts(normalize=True) * 100
validation_distribution = validation_set['Overdue_Flag'].value_counts(normalize=True) * 100
test_distribution = test_set['Overdue_Flag'].value_counts(normalize=True) * 100

# Combine the distributions into a single DataFrame
distribution_df = pd.DataFrame({
    'Overdue_Flag': ['Overdue', 'Not Overdue'],
    'Train': train_distribution.reindex(['Overdue', 'Not Overdue'], fill_value=0).values,
    'Validation': validation_distribution.reindex(['Overdue', 'Not Overdue'], fill_value=0).values,
    'Test': test_distribution.reindex(['Overdue', 'Not Overdue'], fill_value=0).values
})

# Reshape the DataFrame for grouped bar plotting
distribution_df = distribution_df.melt(id_vars='Overdue_Flag', var_name='Set', value_name='Percentage')

# Define custom color mapping
color_map = {'Overdue': '#e74c3c', 'Not Overdue': '#27ae60'}

# Create the grouped bar chart with custom colors
fig = px.bar(
    distribution_df,
    x='Set',
    y='Percentage',
    color='Overdue_Flag',
    barmode='group',
    title='Distribution of Overdue Flag Across Train, Validation, and Test Sets',
    labels={'Set': 'Dataset', 'Percentage': 'Percentage (%)', 'Overdue_Flag': 'Overdue Status'},
    color_discrete_map=color_map,
    text_auto=True
)

# Show the plot
fig.show()

# 7. Feature Engineering
# 7.1. Customer's historical payment behavior


def generate_customer_features(invoices_df, time_window_months):
    """
    Generate customer-level features from invoice data, considering only invoices from the last 'time_window_months' months.

    Parameters:
    - invoices_df (pd.DataFrame): DataFrame containing invoice data.
    - time_window_months (int): The number of months to consider for calculations (e.g., 6, 12, 16).

    Returns:
    - cust_features (pd.DataFrame): DataFrame with customer-level aggregated features.
    """

    # Determine the cutoff date for the time window
    max_date = invoices_df['Clearing_Date'].max()

    # Calculate the cutoff date, ensuring it starts from the first of the month
    cutoff_date = (max_date - pd.DateOffset(months=time_window_months)).replace(day=1)
    print(cutoff_date)

    # Filter invoices within the specified time window
    filtered_invoices = invoices_df[invoices_df['Clearing_Date'] >= cutoff_date]

    # 1) General Features
    cust_features = filtered_invoices.groupby('Payer_Code')['Unique_Key'].count().reset_index()
    cust_features.rename(columns={'Unique_Key': 'Invoices_Paid'}, inplace=True)

    # Total invoice amount
    total_invoice_amount = filtered_invoices.groupby('Payer_Code')['Amount_GC'].sum().reset_index()
    total_invoice_amount.rename(columns={'Amount_GC': 'Total_Amount'}, inplace=True)
    cust_features = pd.merge(cust_features, total_invoice_amount, on='Payer_Code', how='left')

    # Average invoice amount
    average_invoice_amount = filtered_invoices.groupby('Payer_Code')['Amount_GC'].mean().reset_index()
    average_invoice_amount.rename(columns={'Amount_GC': 'Average_Amount'}, inplace=True)
    cust_features = pd.merge(cust_features, average_invoice_amount, on='Payer_Code', how='left')

    # Min & Max invoice amount
    min_invoice_amount = filtered_invoices.groupby('Payer_Code')['Amount_GC'].min().reset_index()
    max_invoice_amount = filtered_invoices.groupby('Payer_Code')['Amount_GC'].max().reset_index()
    min_invoice_amount.rename(columns={'Amount_GC': 'Min_Invoice_Amount'}, inplace=True)
    max_invoice_amount.rename(columns={'Amount_GC': 'Max_Invoice_Amount'}, inplace=True)
    cust_features = pd.merge(cust_features, min_invoice_amount, on='Payer_Code', how='left')
    cust_features = pd.merge(cust_features, max_invoice_amount, on='Payer_Code', how='left')

    # Count of disputes
    disputes_count = filtered_invoices[filtered_invoices['Dispute_Category_Description'] != 'No Dispute'].groupby('Payer_Code')['Dispute_Category_Description'].count().reset_index()
    disputes_count.rename(columns={'Dispute_Category_Description': 'Total_Disputes'}, inplace=True)
    cust_features = pd.merge(cust_features, disputes_count, on='Payer_Code', how='left')

    # 2) Timeliness-Based Features
    average_timeliness = filtered_invoices.groupby('Payer_Code')['Timeliness'].mean().reset_index()
    average_timeliness.rename(columns={'Timeliness': 'Average_Timeliness'}, inplace=True)
    cust_features = pd.merge(cust_features, average_timeliness, on='Payer_Code', how='left')

    # Average paid late days
    average_paid_late_days = filtered_invoices[filtered_invoices['Timeliness'] > 0].groupby('Payer_Code')['Timeliness'].mean().reset_index()
    average_paid_late_days.rename(columns={'Timeliness': 'Average_Paid_Late_Days'}, inplace=True)
    cust_features = pd.merge(cust_features, average_paid_late_days, on='Payer_Code', how='left')

    # Average paid early days
    average_paid_early_days = filtered_invoices[filtered_invoices['Timeliness'] <= 0].groupby('Payer_Code')['Timeliness'].mean().reset_index()
    average_paid_early_days.rename(columns={'Timeliness': 'Average_Paid_Early_Days'}, inplace=True)
    cust_features = pd.merge(cust_features, average_paid_early_days, on='Payer_Code', how='left')

    # Percentage of On-Time & Overdue Payments
    on_time_payments = filtered_invoices[filtered_invoices['Timeliness'] <= 0].groupby(
        'Payer_Code').size().reset_index()
    on_time_payments.rename(columns={0: 'On_Time_Payments'}, inplace=True)
    cust_features = pd.merge(cust_features, on_time_payments, on='Payer_Code', how='left')
    cust_features['On_Time_Payments'] = cust_features['On_Time_Payments'].fillna(0)
    cust_features['Percentage_On_Time_Payments'] = cust_features['On_Time_Payments'] / cust_features['Invoices_Paid']

    not_on_time_payments = filtered_invoices[filtered_invoices['Timeliness'] > 0].groupby(
        'Payer_Code').size().reset_index()
    not_on_time_payments.rename(columns={0: 'Not_On_Time_Payments'}, inplace=True)
    cust_features = pd.merge(cust_features, not_on_time_payments, on='Payer_Code', how='left')
    cust_features['Not_On_Time_Payments'] = cust_features['Not_On_Time_Payments'].fillna(0)
    cust_features['Percentage_Not_On_Time_Payments'] = cust_features['Not_On_Time_Payments'] / cust_features[
        'Invoices_Paid']

    # 3) Overdue Amount Features
    total_overdue_amount = filtered_invoices[filtered_invoices['Overdue_Flag'] == 'Overdue'].groupby('Payer_Code')[
        'Amount_GC'].sum().reset_index()
    total_overdue_amount.rename(columns={'Amount_GC': 'Total_Overdue_Amount'}, inplace=True)
    cust_features = pd.merge(cust_features, total_overdue_amount, on='Payer_Code', how='left')
    cust_features['Total_Overdue_Amount'] = cust_features['Total_Overdue_Amount'].fillna(0)

    cust_features['Percentage_Overdue_Amount'] = cust_features['Total_Overdue_Amount'] / cust_features['Total_Amount']
    cust_features['Percentage_Overdue_Amount'] = cust_features['Percentage_Overdue_Amount'].fillna(0)

    # 4) Temporal Features
    first_invoice_date = filtered_invoices.groupby('Payer_Code')['Document_Date'].min().reset_index()
    last_invoice_date = filtered_invoices.groupby('Payer_Code')['Document_Date'].max().reset_index()
    first_invoice_date.rename(columns={'Document_Date': 'First_Invoice_Date'}, inplace=True)
    last_invoice_date.rename(columns={'Document_Date': 'Last_Invoice_Date'}, inplace=True)
    cust_features = pd.merge(cust_features, first_invoice_date, on='Payer_Code', how='left')
    cust_features = pd.merge(cust_features, last_invoice_date, on='Payer_Code', how='left')

    # Invoice Recency: Days since the last invoice in the selected window
    cust_features['Invoice_Recency'] = (max_date - cust_features['Last_Invoice_Date']).dt.days

    # 5) Interaction Features - Credit Utilization
    credit_limit = filtered_invoices.groupby('Payer_Code')['Credit_Limit'].first().reset_index()
    cust_features = pd.merge(cust_features, credit_limit, on='Payer_Code', how='left')
    cust_features['Credit_Utilization'] = cust_features['Total_Amount'] / cust_features['Credit_Limit']

    # Replace infinite values with 0
    cust_features['Credit_Utilization'] = cust_features['Credit_Utilization'].replace([float('inf'), -float('inf')], 0)

    # Drop unnecessary columns
    cust_features.drop(['First_Invoice_Date', 'Last_Invoice_Date', 'Credit_Limit'], axis=1, inplace=True)

    # Fill NaN values with 0 for all numeric columns
    cust_features = cust_features.fillna(0)

    return cust_features


customer_features = generate_customer_features(train_set, time_window_months=12)
print(customer_features.head())
print(customer_features.isna().sum())

# 7.2. Dummy features

def generate_dummy_features(invoices_df):

    # 1) Dunning Block
    # Transform 'DunningBlock' into binary column
    dummies = invoices_df.copy()
    dummies['Has_Dunning_Block_Flag'] = dummies['DunningBlock_MANSP'].apply(lambda x: 0 if x == 'No Dunning Block' else 1)

    # 2) Overdue Flag
    # Transform 'Overdue_Flag' into binary column
    dummies['Is_Overdue_Flag'] = dummies['Overdue_Flag'].apply(lambda x: 1 if x == 'Overdue' else 0)

    # 3) Customer Contact Type description
    # Create Contacted_Flag
    dummies['Contacted_Flag'] = dummies['Customer_Contact_Type_description'].apply(
        lambda x: 0 if x == 'Not Contacted' else 1)

    # Create specific contact type flags
    dummies['Contacted_By_Email_Flag'] = dummies['Customer_Contact_Type_description'].apply(
        lambda x: 1 if x == 'Email' else 0)

    dummies['Contacted_By_Call_Flag'] = dummies['Customer_Contact_Type_description'].apply(
        lambda x: 1 if x == 'Call' else 0)

    dummies['Contacted_By_Visit_Flag'] = dummies['Customer_Contact_Type_description'].apply(
        lambda x: 1 if x == 'Visit' else 0)

    # 4) Customer Contact Result description

    # Define the set of values indicating successful contact
    successful_contact_values = {'Email', 'Customer Reached', 'contacted by customer'}

    # Create a flag for successfully reached customers
    dummies['Successfully_Reached_Flag'] = dummies['Customer_Contact_Result_description'].apply(
        lambda x: 1 if x in successful_contact_values else 0)

    # 5) Comment Flag Dunning_Flag
    # Transform 'Comment_Flag' into binary column
    dummies['Comment_Flag'] = dummies['Comment_Flag'].apply(lambda x: 1 if x == 'Yes' else 0)

    # Transform 'Dunning_Flag' into binary column
    dummies['Dunning_Flag'] = dummies['Dunning_Flag'].apply(lambda x: 1 if x == 'Yes' else 0)

    # 6) Account Channel Description
    # Define the most important channels
    channels = ['Service Provider', 'End User', 'OEM', 'Dealer', 'Customer Finance', 'Importer']

    # Create flag columns for each channel
    for value in channels:
        column_name = f"{value.replace(' ', '_')}_Flag"  # Create a clean column name
        dummies[column_name] = dummies['AccountChannel_Description'].apply(
            lambda x: 1 if x == value else 0)

    # 7) Country of the customer
    # Filter countries with Invoice Percentage > 5%
    countries_over_5_percent = payment_percentage_by_country_df[payment_percentage_by_country_df['Invoice Percentage'] > 5]['Country']
    countries_over_5_percent_df = countries_over_5_percent.reset_index()
    countries_over_5_percent_df.columns = ['Index','Country']

    # Mapping of country codes to full names
    country_mapping = {
        'USA': 'United States',
        'GBR': 'United Kingdom',
        'DEU': 'Germany',
        'FRA': 'France',
        'SWE': 'Sweden',
        'NLD': 'Netherlands',
    }

    # Replace country codes with full names
    countries_over_5_percent_df['Country'] = countries_over_5_percent_df['Country'].map(country_mapping)

    # Create a flag column for each country
    dummies['Country_USA'] = dummies['Country_ISO3'].apply(lambda x: 1 if x == 'USA' else 0)
    dummies['Country_UK'] = dummies['Country_ISO3'].apply(lambda x: 1 if x == 'GBR' else 0)
    dummies['Country_Germany'] = dummies['Country_ISO3'].apply(lambda x: 1 if x == 'DEU' else 0)
    dummies['Country_France'] = dummies['Country_ISO3'].apply(lambda x: 1 if x == 'FRA' else 0)
    dummies['Country_Sweden'] = dummies['Country_ISO3'].apply(lambda x: 1 if x == 'SWE' else 0)
    dummies['Country_Netherlands'] = dummies['Country_ISO3'].apply(lambda x: 1 if x == 'NLD' else 0)

    # 8) Dispute Category Description
    # Create Disputed_Flag
    dummies['Disputed_Flag'] = dummies['Dispute_Category_Description'].apply(
        lambda x: 0 if x == 'No Dispute' else 1)

    # Create specific dispute categories flags
    dummies['Disputed_Logistics_Returns_Flag'] = dummies['Dispute_Category_Description'].apply(
        lambda x: 1 if x == 'Logistics / Returns' else 0)

    dummies['Disputed_Price_Sales_Flag'] = dummies['Dispute_Category_Description'].apply(
        lambda x: 1 if x == 'Price / Sales' else 0)

    dummies['Disputed_Administration_Flag'] = dummies['Dispute_Category_Description'].apply(
        lambda x: 1 if x == 'Administration' else 0)

    # 9) Promise State Description
    # Create Promise_Flag
    dummies['Promise_Flag'] = dummies['State_Description'].apply(
        lambda x: 0 if x == 'Not Contacted' else 1)

    # Create broken promise flags
    dummies['Broken_Promise_Flag'] = dummies['State_Description'].apply(
        lambda x: 1 if x == 'Broken' else 0)

    # 10) Interest Flag
    dummies['Interest_Flag'] = dummies['Interest_Flag'].apply(
        lambda x: 0 if x == 'No Interest Charged' else 1)

    # 11) Due date falling on a weekend
    # Create a flag for weekends (Saturday = 5, Sunday = 6)
    dummies['Due_Date_Is_Weekend'] = dummies['DueDate'].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)

    # 12) Lagged variable for the Timeliness and the Target (Overdue_Flag) column based on the previously cleared invoice for each payer
    # Sort by Payer_Code and Clearing_Date to ensure proper order
    dummies = dummies.sort_values(by=['Payer_Code', 'Clearing_Date'])

    # Create a lagged columns (previous cleared invoice)
    dummies['Timeliness_Lagged'] = dummies.groupby('Payer_Code')['Timeliness'].shift(1)
    dummies['Is_Overdue_Flag_Lagged'] = dummies.groupby('Payer_Code')['Is_Overdue_Flag'].shift(1)

    # Fill NaN values (cases where no previous invoice exists) with 0
    dummies['Timeliness_Lagged'] = dummies['Timeliness_Lagged'].fillna(0)
    dummies['Is_Overdue_Flag_Lagged'] = dummies['Is_Overdue_Flag_Lagged'].fillna(0)

    # 13) Seasonal Trends
    # Extract Invoice Month
    dummies['Invoice_Month'] = dummies['Document_Date'].dt.month

    # Extract Invoice Quarter
    dummies['Invoice_Quarter'] = dummies['Document_Date'].dt.quarter

    # Drop the useless columns
    dummies = dummies.drop(['DunningBlock_MANSP', 'Overdue_Flag', 'Customer_Contact_Type_description',
                                          'Customer_Contact_Result_description', 'Comment', 'AccountChannel_Description', 'Country_LAND1',
                                          'Dispute_Category_Description', 'Dispute_Reason_Description', 'State_Description', 'Country_ISO3', 'Effect_Flag'], axis=1)

    return dummies

# 7.3 Log-Transformed features

def generate_log_transformed_features(invoices_df):

    # 1) Amount GC
    invoices_df['Log_Amount_GC'] = invoices_df['Amount_GC'].apply(lambda x: np.log(x + 1))  # Adding 1 to avoid log(0)

    # 2) Credit limit
    invoices_df['Log_Credit_Limit'] = invoices_df['Credit_Limit'].apply(lambda x: np.log(x + 1))  # Adding 1 to avoid log(0)

    return invoices_df

# 8. Transforming the train, validation and test datasets

# 8.1. Train dataset
dummified_train_set = generate_dummy_features(train_set)
train_df = generate_log_transformed_features(dummified_train_set)

# Merge the Customer features with the final train set
final_train_set = train_df.merge(customer_features, on='Payer_Code', how='left')

# X, Y training set
X_train = final_train_set.drop(['Company_Code', 'Accounting_Document', 'Document_Date', 'Posting_Date','DueDate',
                                        'Group_Currency', 'Clearing_Date', 'PayT_Code', 'Unique_Key','Amount_GC',
                                        'Credit_Limit', 'Comment_Flag', 'Is_Overdue_Flag', 'Timeliness_Description', 'Timeliness'], axis=1)

y_train = final_train_set['Is_Overdue_Flag']

# Fill NaNs with 0
X_train = X_train.fillna(0)
print(X_train.isna().sum())

# 8.2. Validation dataset
dummified_validation_set = generate_dummy_features(validation_set)
validation_df = generate_log_transformed_features(dummified_validation_set)

# Merge the Customer features with the final validation set
final_validation_set = validation_df.merge(customer_features, on='Payer_Code', how='left')

# X, Y Validation set

X_validation = final_validation_set.drop(['Company_Code', 'Accounting_Document', 'Document_Date', 'Posting_Date','DueDate',
                                        'Group_Currency', 'Clearing_Date', 'PayT_Code', 'Unique_Key','Amount_GC',
                                        'Credit_Limit', 'Comment_Flag', 'Is_Overdue_Flag', 'Timeliness_Description', 'Timeliness'], axis=1)

y_validation = final_validation_set['Is_Overdue_Flag']

# New customers are present in the validation dataset, replace missing values with global statistics (median):
print(X_validation.isna().sum())

# # Fill NaNs with the median of the corresponding feature from training data
# for col in customer_features.columns:
#     if col in X_validation.columns:
#         X_validation[col] = X_validation[col].fillna(customer_features[col].median())

# Fill NaNs with 0
X_validation = X_validation.fillna(0)
print(X_validation.isna().sum())

# 8.3. Test dataset
dummified_test_set = generate_dummy_features(test_set)
test_df = generate_log_transformed_features(dummified_test_set)

# Merge the Customer features with the final validation set
final_test_set = test_df.merge(customer_features, on='Payer_Code', how='left')

# X, Y Test set

X_test = final_test_set.drop(['Company_Code', 'Accounting_Document', 'Document_Date', 'Posting_Date','DueDate',
                                        'Group_Currency', 'Clearing_Date', 'PayT_Code', 'Unique_Key','Amount_GC',
                                        'Credit_Limit', 'Comment_Flag', 'Is_Overdue_Flag', 'Timeliness_Description', 'Timeliness'], axis=1)

y_test = final_test_set['Is_Overdue_Flag']

# New customers are present in the test dataset, replace missing values with global statistics (median):
print(X_test.isna().sum())

# # Fill NaNs with the median of the corresponding feature from training data
# for col in customer_features.columns:
#     if col in X_test.columns:
#         X_test[col] = X_test[col].fillna(customer_features[col].median())

# Fill NaNs with 0
X_test = X_test.fillna(0)
print(X_test.isna().sum())

# 8. Correlation Analysis

# Calculate the correlation matrix excluding the 'Payer_Code' column
corr = X_train.drop(columns=['Payer_Code']).corr()

# Convert the correlation matrix to a long-form DataFrame for Plotly
corr_long = corr.reset_index().melt(id_vars='index', var_name='Feature2', value_name='Correlation')
corr_long.rename(columns={'index': 'Feature1'}, inplace=True)


# Calculate correlation between features and 'Days_to_Payment' (DTP)
correlations = X_train.drop(columns=['Payer_Code']).corrwith(y_train).sort_values(ascending=False)

# Convert to DataFrame for visualization
corr_df = pd.DataFrame({
    'Feature': correlations.index,
    'Correlation': correlations.values
}).sort_values(by='Correlation', ascending=True)  # Sort in ascending order

# Create the horizontal bar plot
fig = px.bar(
    corr_df,
    x='Correlation',
    y='Feature',
    orientation='h',
    title="Correlation Scores Between Features and Target Variable",
    labels={'Correlation': 'Correlation Value', 'Feature': 'Features'},
    color='Correlation',
    color_continuous_scale=['#ff6200', '#ffcaa8', 'white', '#a8ffc3', '#00b050']  # Red to Green gradient
)

# Adjust layout
fig.update_layout(
    width=900, height=600,
    xaxis_title="Correlation Value",
    yaxis_title="Features",
    showlegend=False
)
fig.show()

# There are some pairs of variables that have high correlations, implying a degree of multicollinearity.

# 9. Feature Scaling
# Initialize the MinMaxScaler
scaler = MinMaxScaler()
# scaler = StandardScaler()

# List of columns that don't need to be scaled
columns_to_exclude = []

# List of columns that need to be scaled
columns_to_scale = X_train.columns.difference(columns_to_exclude)

# Copy the cleaned dataset
X_train_scaled = X_train.copy()
X_val_scaled = X_validation.copy()
X_test_scaled = X_test.copy()

# Applying the scaler to the necessary columns in the dataset
X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train_scaled[columns_to_scale])
X_val_scaled[columns_to_scale] = scaler.fit_transform(X_val_scaled[columns_to_scale])
X_test_scaled[columns_to_scale] = scaler.fit_transform(X_test_scaled[columns_to_scale])

# Display the first few rows of the scaled data
X_train_scaled.head(5)
X_val_scaled.head(5)
X_test_scaled.head(5)

# 10. Model training
# Function to build the neural network model
def build_nn_model(input_dim):
    model = Sequential([
        Dense(64, kernel_initializer='he_normal', input_shape=(input_dim,)),
        BatchNormalization(),
        LeakyReLU(alpha=0.1),  # Use LeakyReLU instead of ReLU
        Dropout(0.3),

        Dense(32, kernel_initializer='he_normal'),
        BatchNormalization(),
        LeakyReLU(alpha=0.1),
        Dropout(0.2),

        Dense(16, kernel_initializer='he_normal'),
        BatchNormalization(),
        LeakyReLU(alpha=0.1),

        Dense(1, activation='sigmoid')  # Binary classification
    ])

    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy', keras.metrics.AUC()])

    return model

# Step 1: Define models
models = {
    "Logistic Regression": LogisticRegression(penalty='l2',max_iter=1000, random_state=42),
    # "Decision Tree": DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=5, random_state=42),
    "Decision Tree": DecisionTreeClassifier(criterion='entropy', min_samples_leaf=1, max_depth=5, min_samples_split=2, random_state=42), # the best model tunned with Grid Search
    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=2, criterion='gini', random_state=42, bootstrap=True), # AUC 0.9121
    # "Random Forest": RandomForestClassifier(n_estimators=1000,  max_depth=15, min_samples_split=5, min_samples_leaf=2, bootstrap=True, random_state=42),
    # "Random Forest": RandomForestClassifier(n_estimators=10, max_depth=12, min_samples_split=10, min_samples_leaf=5, bootstrap=True, random_state=42),
    # "XGBoost": XGBClassifier(n_estimators=1000, learning_rate=0.005, max_depth=6, subsample=0.8, eval_metric='logloss', random_state=42),
    # "XGBoost": XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, eval_metric='logloss', random_state=42),
    # "XGBoost": XGBClassifier(n_estimators=10,
    # learning_rate=0.005, max_depth=4, subsample=0.7, reg_lambda=10, random_state=42),
    # "LightGBM": LGBMClassifier(n_estimators=1000, learning_rate=0.005, random_state=42),
    "LightGBM": LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=-1, num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42),
    # "LightGBM": LGBMClassifier( n_estimators=500, learning_rate=0.005, max_depth=8, subsample=0.8, reg_lambda=10, min_child_samples=2, random_state=42),
    # "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
    # "CatBoost": CatBoostClassifier(iterations=300, learning_rate=0.01, l2_leaf_reg=2, depth=8, verbose=0, random_state=42)
    "CatBoost": CatBoostClassifier(iterations=100, depth=6, learning_rate=0.01, l2_leaf_reg=2, verbose=0, random_state=42), # AUC: 0.9135
    # "CatBoost": CatBoostClassifier(iterations=500, depth=6, learning_rate=0.01, verbose=0, random_state=42), # the best so far
    # "CatBoost": CatBoostClassifier(iterations=500, learning_rate=0.03, depth=8, l2_leaf_reg=3, random_state=42,  verbose=0),
    # "CatBoost": CatBoostClassifier(iterations=10, learning_rate=0.005, depth=6, l2_leaf_reg=10, subsample=0.7, random_state=42, verbose=0),
    # "Gradient Boosting (GBM)": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    "Gradient Boosting (GBM)": GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=5, subsample=0.8, random_state=42),
    # "Neural Network": KerasClassifier(model=build_nn_model, input_dim=X_train.shape[1], epochs=50, batch_size=32, verbose=1),
    "KNN": KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2),
    # "KNN": KNeighborsClassifier(n_neighbors=10, weights='distance', metric='manhattan')
}

# Step 2: Train, evaluate, and plot ROC curves
# Dictionary to store trained models and their performance
trained_models = {}  # Store models
roc_results = {}  # Store ROC AUC results

# Initialize Plotly ROC figure
fig = go.Figure()

for name, model in models.items():
    print(f"Training {name}...")

    # Train the model
    model.fit(X_train_scaled, y_train)

    # Store trained model in a dictionary
    trained_models[name] = model

    # Predict probabilities for ROC curve
    y_train_prob = model.predict_proba(X_train_scaled)[:, 1]  # For training
    y_val_prob = model.predict_proba(X_val_scaled)[:, 1]      # For validation

    # Predict labels for train & validation sets
    y_train_pred = model.predict(X_train_scaled)
    y_val_pred = model.predict(X_val_scaled)

    # Calculate ROC and AUC
    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_prob)
    roc_auc_train = auc(fpr_train, tpr_train)

    fpr_val, tpr_val, _ = roc_curve(y_validation, y_val_prob)
    roc_auc_val = auc(fpr_val, tpr_val)

    # Store AUC results for later comparison
    roc_results[name] = {
        "Train AUC": roc_auc_train,
        "Validation AUC": roc_auc_val
    }

    # Print classification report for train and validation set
    print(f"\n{name} Training Classification Report:\n")
    print(classification_report(y_train, y_train_pred, digits=4))

    print(f"\n{name} Validation Classification Report:\n")
    print(classification_report(y_validation, y_val_pred, digits=4))

    # Add ROC curve for validation set
    fig.add_trace(go.Scatter(
        x=fpr_val, y=tpr_val,
        mode='lines',
        name=f"{name} (AUC = {roc_auc_val:.4f})"
    ))

# Add diagonal reference line
fig.add_trace(go.Scatter(
    x=[0, 1], y=[0, 1],
    mode='lines',
    line=dict(color='gray', dash='dash'),
    name='Random Guess'
))

# Update ROC Curve layout
fig.update_layout(
    title="ROC Curve Comparison (Validation Set)",
    xaxis_title="False Positive Rate",
    yaxis_title="True Positive Rate",
    legend_title="Models",
    width=900,
    height=600,
    template='plotly_white'
)

# Show ROC Curve
fig.show()

# Step 3: Evaluate on the Test Set
# Initialize figure for ROC Curves
fig_roc = go.Figure()
print("\nTest Set Evaluation:\n")

# Dictionary to store confusion matrices for all models
confusion_matrices = {}

for name, model in models.items():
    print(f"Evaluating {name} on the Test Set...")

    # Predict probabilities for ROC curve
    try:
        y_test_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probabilities
    except AttributeError:  # For models like KerasClassifier that may not have `predict_proba`
        y_test_prob = model.predict(X_test_scaled).flatten()

    # Predict labels for test set
    y_test_pred = model.predict(X_test_scaled)

    # Calculate ROC and AUC
    fpr, tpr, _ = roc_curve(y_test, y_test_prob)
    roc_auc = auc(fpr, tpr)

    # Print classification report
    print(f"\n{name} Test Classification Report:\n")
    print(classification_report(y_test, y_test_pred, digits=4))

    # Print AUC score
    print(f"{name} Test AUC: {roc_auc:.4f}\n")

    # Add model ROC curve to Plotly figure
    fig_roc.add_trace(go.Scatter(
        x=fpr,
        y=tpr,
        mode='lines',
        name=f"{name} (AUC = {roc_auc:.4f})"
    ))

    # Compute and store the confusion matrix
    cm = confusion_matrix(y_test, y_test_pred)
    confusion_matrices[name] = cm

# Add diagonal reference line (Random Guess) to ROC curve
fig_roc.add_trace(go.Scatter(
    x=[0, 1],
    y=[0, 1],
    mode='lines',
    line=dict(dash='dash'),
    name="Random Guess (AUC = 0.50)"
))

# Customize layout for ROC curve
fig_roc.update_layout(
    title="ROC Curve Comparison Across Models",
    xaxis_title="False Positive Rate",
    yaxis_title="True Positive Rate",
    legend=dict(x=0.7, y=0.05),
    width=900,
    height=600
)
fig_roc.show()

# Plot Confusion Matrices for Each Model
for name, cm in confusion_matrices.items():
        cm = cm[::-1]

        # Create confusion matrix heatmap using Plotly
        fig_cm = ff.create_annotated_heatmap(
            z=cm,
            x=['Not Overdue', 'Overdue'],  # ** Actual labels (X-axis)**
            y=['Overdue', 'Not Overdue'],  # **Predicted labels (Y-axis)**
            showscale=True
        )

        # Customize layout for confusion matrix
        fig_cm.update_layout(
            title=f"Confusion Matrix: {name}",
            xaxis_title="Actual Label",
            yaxis_title="Predicted Label"
        )

        # Show the interactive confusion matrix plot
        fig_cm.show()

# Step 4: Implementing Hyperparameter Tuning with GridSearchCV
# Define the parameter grid for Decision Tree
param_grid_dt = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

# Initialize GridSearchCV with Decision Tree Classifier
grid_search_dt = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid_dt,
    cv=3,  # 3-fold cross-validation
    scoring='accuracy',  # Optimize for accuracy
#    n_jobs=-1,  # Use all CPU cores
    verbose=1  # Show progress
)

# Fit the grid search on training data
grid_search_dt.fit(X_train_scaled, y_train)

# Print best hyperparameters and best accuracy score
print("Best Parameters for Decision Tree:", grid_search_dt.best_params_)
print("Best Accuracy for Decision Tree:", grid_search_dt.best_score_)

# Step 5. Feature Importance
# Dictionary to store feature importance
feature_importance_dict = {}

# List of models that have `feature_importances_`
tree_based_models = ["Decision Tree", "Random Forest", "LightGBM", "CatBoost", "Gradient Boosting (GBM)"]

# Iterate through models
for name, model in models.items():

    if name in tree_based_models:
        # Extract feature importance from tree-based models
        importance = model.feature_importances_
        feature_importance_dict[name] = pd.DataFrame({
            "Feature": X_train.columns,
            "Importance": importance
        }).sort_values(by="Importance", ascending=False)

    elif name == "Logistic Regression":
        # Extract importance from coefficients (absolute values)
        importance = abs(model.coef_[0])
        feature_importance_dict[name] = pd.DataFrame({
            "Feature": X_train.columns,
            "Importance": importance
        }).sort_values(by="Importance", ascending=False)


# Visualize feature importance
for model_name, df in feature_importance_dict.items():
    # Take top 25 most important features
    df_top = df.head(25)

    # Sort the features in ascending order
    df_top = df_top.sort_values(by='Importance', ascending=True)

    # Plot
    fig = px.bar(
        df_top,
        x="Importance",
        y="Feature",
        orientation='h',
        title=f"Feature Importance - {model_name}",
        labels={"Importance": "Importance Score", "Feature": "Features"},
        color="Importance",
        color_continuous_scale="viridis",
        text="Importance"
    )

    # Format text to display with 4 decimal places
    fig.update_traces(texttemplate='%{text:.4f}', textposition='outside')

    # Show the plot
    fig.show()

# Decision Tree model - visualization

def extract_tree_graph(tree, feature_names):
    """
    Extracts the tree structure for Plotly visualization.

    Parameters:
    - tree: Trained DecisionTreeClassifier.
    - feature_names: List of feature names.

    Returns:
    - nodes, edges: Lists containing graph information for visualization.
    """
    tree_ = tree.tree_
    nodes = []
    edges = []

    def recurse(node, parent, depth, x, y, offset):
        """Recursive function to extract tree structure"""
        samples = tree_.n_node_samples[node]  # Ensure 'samples' is always defined

        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            feature = feature_names[tree_.feature[node]]
            threshold = tree_.threshold[node]
            gini = tree_.impurity[node]
            value = tree_.value[node][0]
            class_label = np.argmax(value)

            node_label = f"{feature} ≤ {threshold:.2f}<br>Samples: {samples}<br>Gini: {gini:.2f}<br>Class: {class_label}"
            nodes.append(dict(id=node, label=node_label, x=x, y=y))

            # Left child (True branch)
            left_child = tree_.children_left[node]
            edges.append(dict(from_node=node, to_node=left_child))
            recurse(left_child, node, depth + 1, x - offset, y - 1, offset / 2)

            # Right child (False branch)
            right_child = tree_.children_right[node]
            edges.append(dict(from_node=node, to_node=right_child))
            recurse(right_child, node, depth + 1, x + offset, y - 1, offset / 2)
        else:
            value = tree_.value[node][0]
            class_label = np.argmax(value)
            node_label = f"Leaf<br>Samples: {samples}<br>Class: {class_label}"
            nodes.append(dict(id=node, label=node_label, x=x, y=y))

    recurse(0, None, 0, 0, 0, 4)
    return nodes, edges


# Extract tree structure
decision_tree = models["Decision Tree"]
feature_names = X_train.columns.tolist()
nodes, edges = extract_tree_graph(decision_tree, feature_names)

# Extract X, Y positions for plotting
x_vals = [node["x"] for node in nodes]
y_vals = [node["y"] for node in nodes]
labels = [node["label"] for node in nodes]

# Create scatter plot for tree nodes
fig = go.Figure()

fig.add_trace(go.Scatter(
    x=x_vals,
    y=y_vals,
    mode="markers+text",
    marker=dict(size=30, color="lightblue", line=dict(color="black", width=2)),
    text=labels,
    textposition="middle center",
    hoverinfo="text"
))

# Draw tree edges
for edge in edges:
    parent_node = next(node for node in nodes if node["id"] == edge["from_node"])
    child_node = next(node for node in nodes if node["id"] == edge["to_node"])

    fig.add_trace(go.Scatter(
        x=[parent_node["x"], child_node["x"]],
        y=[parent_node["y"], child_node["y"]],
        mode="lines",
        line=dict(color="black", width=1),
        hoverinfo="skip"
    ))

# Update layout
fig.update_layout(
    title="Decision Tree Visualization",
    showlegend=False,
    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
    height=800,
    width=1000
)

fig.show()


